{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEXk0B1m15XV"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "from nltk.tree import Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_J6EBQ63em-",
        "outputId": "6787ab4e-afe5-4dc1-f430-e87467f2f112"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('maxent_ne_chunker_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UY5HqGzq2FIX"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRt8Kag_2Hrh",
        "outputId": "12a23534-d2ba-4938-e5cb-6d82ae64648b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYKZvogk2KTL",
        "outputId": "3b57d1b9-5f73-4ac1-b5c6-78361f5ef2f1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated transcript written to /content/drive/My Drive/Research/RGU/updated_transcript3.txt\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "from nltk.tree import Tree\n",
        "\n",
        "# Download necessary NLTK datasets if not already present\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "# Read the file\n",
        "def read_transcript(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "    return lines\n",
        "\n",
        "# Extracts named entities (Person names) using NLTK\n",
        "def extract_named_entities(lines):\n",
        "    named_entities = set()\n",
        "    for line in lines:\n",
        "        match = re.match(r\"(Speaker\\d+): (.+)\", line)  # Extract only the text after \"SpeakerX:\"\n",
        "        if match:\n",
        "            _, text = match.groups()\n",
        "            tokens = word_tokenize(text)\n",
        "            tags = pos_tag(tokens)\n",
        "\n",
        "            chunks = ne_chunk(tags)\n",
        "            for chunk in chunks:\n",
        "                if isinstance(chunk, Tree) and chunk.label() == \"PERSON\":\n",
        "                    name = \" \".join(c[0] for c in chunk)  # multi-word names combining\n",
        "                    named_entities.add(name)\n",
        "    return list(named_entities)\n",
        "\n",
        "# Filters extracted named entities to associate speaker labels\n",
        "def filter_speakers(lines, named_entities):\n",
        "    speaker_mapping = {}\n",
        "    seen_names = set()  # To prevent duplicate assignments\n",
        "\n",
        "    for line in lines:\n",
        "        match = re.match(r\"(Speaker\\d+): (.+)\", line)\n",
        "        if match:\n",
        "            speaker, text = match.groups()\n",
        "\n",
        "            # Check for direct name introductions\n",
        "            name_intro_match = re.search(r\"my name is ([A-Z][a-z]+(?: [A-Z][a-z]+)?)\", text, re.IGNORECASE)\n",
        "            if name_intro_match:\n",
        "                name = name_intro_match.group(1)\n",
        "                if name not in seen_names:  # For unique names\n",
        "                    speaker_mapping[speaker] = name\n",
        "                    seen_names.add(name)\n",
        "                    continue\n",
        "\n",
        "            # Check for \"That's me\" introductions and ensure the correct speaker is assigned\n",
        "            thats_me_match = re.search(r\"([A-Z][a-z]+ [A-Z][a-z]+),? that'?s me\", text, re.IGNORECASE)\n",
        "            if thats_me_match:\n",
        "                name = thats_me_match.group(1)\n",
        "                if name not in seen_names:\n",
        "                    speaker_mapping[speaker] = name\n",
        "                    seen_names.add(name)\n",
        "                    continue\n",
        "\n",
        "            # Check if the speaker is mentioned by others\n",
        "            for name in named_entities:\n",
        "                if name in text and name not in seen_names:\n",
        "                    speaker_mapping[speaker] = name\n",
        "                    seen_names.add(name)\n",
        "                    break\n",
        "\n",
        "            # If no name found from other methods\n",
        "            if speaker not in speaker_mapping:\n",
        "                speaker_mapping[speaker] = f\"Unknown Speaker {len(speaker_mapping) + 1}\"\n",
        "\n",
        "    return speaker_mapping\n",
        "\n",
        "# Replaces SpeakerX labels with corresponding names\n",
        "def replace_speaker_labels(lines, speaker_mapping):\n",
        "    updated_lines = []\n",
        "    for line in lines:\n",
        "        match = re.match(r\"(Speaker\\d+): (.+)\", line)\n",
        "        if match:\n",
        "            speaker, text = match.groups()\n",
        "            if speaker in speaker_mapping:\n",
        "                updated_lines.append(f\"{speaker_mapping[speaker]}: {text}\\n\")\n",
        "            else:\n",
        "                updated_lines.append(line)\n",
        "        else:\n",
        "            updated_lines.append(line)  # Keep non-matching lines (e.g., blank lines)\n",
        "    return updated_lines\n",
        "\n",
        "# Update transcript\n",
        "def write_transcript(output_path, updated_lines):\n",
        "    with open(output_path, 'w') as file:\n",
        "        file.writelines(updated_lines)\n",
        "\n",
        "def main():\n",
        "    # File paths\n",
        "    input_path = \"/content/drive/My Drive/Research/RGU/ner-transcript.txt\"\n",
        "    output_path = \"/content/drive/My Drive/Research/RGPU/updated_transcript.txt\"\n",
        "\n",
        "    # Workflow\n",
        "    lines = read_transcript(input_path)\n",
        "    named_entities = extract_named_entities(lines)\n",
        "    speaker_mapping = filter_speakers(lines, named_entities)\n",
        "    updated_lines = replace_speaker_labels(lines, speaker_mapping)\n",
        "    write_transcript(output_path, updated_lines)\n",
        "\n",
        "    print(f\"Updated transcript written to {output_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIGBa52s2aVm"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}